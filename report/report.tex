\documentclass[a4paper,titlepage]{article}
\usepackage[utf8]{inputenc}
\usepackage{fullpage}
\usepackage{indentfirst}
\usepackage[per-mode=symbol]{siunitx}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{color}
\usepackage{amsmath}
\usepackage{array}
\usepackage[hidelinks]{hyperref}
\usepackage[format=plain,font=it]{caption}
\usepackage{subcaption}
\usepackage{standalone}
\usepackage[nottoc]{tocbibind}
\usepackage[noabbrev,capitalize,nameinlink]{cleveref}
\usepackage{titlesec}
\usepackage{booktabs}
\usepackage{csvsimple}
\usepackage[super]{nth}

% Custom commands
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand{\code}[1]{\texttt{#1}}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}

\titleformat*{\section}{\normalsize\bfseries}

%opening
\title{
	\textbf{ECSE 526 \\ Assignment 3}
	\\ \large Reinforcement Learning
}
\author{Sean Stappas \\ 260639512}
\date{November \nth{7}, 2017}

\begin{document}
	\sloppy
	\maketitle
	\twocolumn
	
	\section*{Introduction}
	
	
	\section{Description of approach to generalization}
	% 10. provides expression for distance metric between two states and describes state representation used by the RL agent, also includes rationale for choice of components of the distance metric
	
	
	\section{Results of generalization}
	% 10. results provided for at least 3 different generalization approaches (i.e., choice of components of the distance metric) and meaningful discussion regarding consequences to behaviour of game agent
	
	
	\section{Description of approach to exploration}
	% 10 provides expression for optimistic prior for (state, action) pairs with clear explanation of how agent chose action at each step and convincing rationale for the approach taken
	
	First, a simple random approach to exploration was implemented, where a random action is chosen with probability $\epsilon$ and the greedy action is chosen otherwise.
	
	% TODO: Tabulate score/level/screenshots for epsilon exploration
	
	Next, weighting with optimistic priors was implemented, using $N(s, a)$, i.e., the number of times action $a$ has been attempted in state $s$.
	
	% TODO: Tabulate score/level/screenshots for optimistic prior
	
	
	\section{Results of exploration}
	% 10. results provided for at least 2 different exploration functions (i.e., weighting or N[s,a] in optimistic prior calculation) and meaningful discussion regarding consequences to behaviour of game agent
	
	The random exploration results can be seen in...
	
	% TODO: Tabulate score/level/screenshots for epsilon exploration
	
	The weighted prior results can be seen in...
	
	% TODO: Tabulate score/level/screenshots for optimistic prior
	
	
	\section{Agent Performance}
	% 30. as above, including analysis of effects of game events on agent behaviour and strategies (e.g., enemy avoidance) AND explanation of results over trials with multiple seeds to demonstrate generalization of learning
	
	% TODO: Show score as function of number of games played (with multiple seeds, game events [enemies])
	
	% TODO: Explain how game events were handled (with subsumption diagram and suppressor nodes, agents taking precedence over blocks...)

	\section*{Conclusion}
	
	
	\section{Acknowledgments}
	
	There was a discussion with Andrei Purcarus and Andrew Lowther concerning the positions of various important bytes in the ALE RAM, including the bytes indicating a safe move update and the byte indicating a level change.
	
	
	% 
	
	%\renewcommand\refname{}
	%\bibliographystyle{unsrt}
	%\bibliography{readings}{}
	
\end{document}
